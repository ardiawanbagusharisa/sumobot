{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a948356",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from dataset_helper import export_dataset, get_dataset, find_project_root, get_dataset_dir\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Amount of dataset lines that will be compiled and converted to dataset.jsonl. \n",
    "# If -1, use all lines.\n",
    "max_dataset=100\n",
    "# max_dataset=-1\n",
    "\n",
    "# Model parameters\n",
    "block_size = 128\n",
    "batch_size = 256\n",
    "n_embed = 192\n",
    "n_heads = 4\n",
    "n_layers = 3\n",
    "lr = 1e-4\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "\n",
    "onnx_output_name=\"slm\"\n",
    "dataset_output_path = f\"{get_dataset_dir()}/slm_dataset.txt\"\n",
    "tokenizer_output_path = f\"{find_project_root()}/Resources/ML/Tokenizer/action_tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2304b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e14182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# To use existing dataset, use dataset_dir param\n",
    "df, dir = get_dataset(prefer_local=False)\n",
    "\n",
    "if max_dataset>-1:\n",
    "    df = df.sample(max_dataset)\n",
    "    \n",
    "export_dataset(df, dataset_output_path, format=\"txt\", completion_mode=\"short\")\n",
    "\n",
    "print(f\"Saved {len(df)} samples to {dataset_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4593d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "specials = [\"<PAD>\"]\n",
    "# Tokenization - Train\n",
    "trainer = trainers.BpeTrainer(special_tokens=specials)\n",
    "tokenizer.train([dataset_output_path], trainer)\n",
    "for tok in specials:\n",
    "    tokenizer.add_special_tokens([tok])\n",
    "# Tokenization - Save to file\n",
    "tokenizer.save(tokenizer_output_path)\n",
    "print(f\"âœ… Tokenizer saved to {tokenizer_output_path}\")\n",
    "\n",
    "\n",
    "# Load Tokenizer\n",
    "data_file = open(dataset_output_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer_output_path)\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc1f919",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def line_token_stream(file):\n",
    "    for line in file:\n",
    "        tokens = tokenizer.encode(line).ids\n",
    "        yield tokens\n",
    "\n",
    "token_stream = line_token_stream(data_file)\n",
    "\n",
    "def get_batch_linear():\n",
    "    global token_stream\n",
    "    x_batch, y_batch = [], []\n",
    "\n",
    "    while len(x_batch) < batch_size:\n",
    "        try:\n",
    "            tokens = next(token_stream)\n",
    "        except StopIteration:\n",
    "            # Restart from beginning\n",
    "            data_file.seek(0)\n",
    "            token_stream = line_token_stream(data_file)\n",
    "            tokens = next(token_stream)\n",
    "\n",
    "        # Pad or trim\n",
    "        if len(tokens) < block_size + 1:\n",
    "            tokens += [0] * (block_size + 1 - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:block_size + 1]\n",
    "\n",
    "        x_batch.append(tokens[:-1])\n",
    "        y_batch.append(tokens[1:])\n",
    "\n",
    "    return (\n",
    "        torch.tensor(x_batch, dtype=torch.long, device=device),\n",
    "        torch.tensor(y_batch, dtype=torch.long, device=device)\n",
    "    )\n",
    "\n",
    "# === Model ===\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) / (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = self.value(x)\n",
    "        return wei @ v\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.proj(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.pos_embedding(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=50):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d8d70",
   "metadata": {},
   "source": [
    "# Training and save to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = GPT().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for step in tqdm(range(max_iters)):\n",
    "    xb, yb = get_batch_linear()  # sequential batching\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        val_x, val_y = get_batch_linear()  # sequential validation\n",
    "        _, val_loss = model(val_x, val_y)\n",
    "        print(f\"Step {step}: train loss {loss.item():.4f}, val loss {val_loss.item():.4f}\")\n",
    "\n",
    "# Generation\n",
    "# Encode example prompt\n",
    "context_ids = tokenizer.encode(\"BotPos=[2.23,2.25], BotRot=228, EnemyPos=[2.87,0.39], EnemyRot=87, AngleToEnemy=-29.68, AngleToEnemyScore=0.87, DistanceToEnemyScore=0.79, NearBorderArenaScore=0.42, FacingToArena=0.65.\").ids\n",
    "context = torch.tensor(context_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "# Generate\n",
    "output_ids = model.generate(context, max_new_tokens=20)[0].tolist()\n",
    "\n",
    "# Decode generated IDs back to text\n",
    "output_text = tokenizer.decode(output_ids)\n",
    "print(output_text)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Dummy input: batch=1, variable sequence length (start small for export)\n",
    "dummy_input = torch.randint(0, vocab_size, (1, 8), dtype=torch.long, device=device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    f\"{onnx_output_name}.onnx\",\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "    },\n",
    "    opset_version=13\n",
    ")\n",
    "\n",
    "print(f\"âœ… Exported GPT model to {onnx_output_name}.onnx\")\n",
    "\n",
    "# Quantize the model - OPTIONAL\n",
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "model = onnx.load(f\"{onnx_output_name}.onnx\")\n",
    "fp16_model = float16.convert_float_to_float16(model)\n",
    "onnx.save(fp16_model, f\"{onnx_output_name}_fp16.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc575b",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e1d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from tokenizers import Tokenizer\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Load trained BPE tokenizer\n",
    "tokenizer = Tokenizer.from_file(tokenizer_output_path)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(f\"{onnx_output_name}.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def generate_onnx(prompt, max_new_tokens=20, block_size=128):\n",
    "    # Encode with BPE tokenizer\n",
    "    input_ids = tokenizer.encode(prompt).ids\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Keep only last block_size tokens\n",
    "        input_slice = input_ids[-block_size:]\n",
    "        input_array = np.array([input_slice], dtype=np.int64)\n",
    "\n",
    "        # Run inference\n",
    "        outputs = session.run(None, {\"input_ids\": input_array})\n",
    "        logits = outputs[0]  # shape: (1, seq_len, vocab_size)\n",
    "\n",
    "        # Get last token logits\n",
    "        next_token_logits = logits[0, -1]\n",
    "        next_token_id = int(np.argmax(next_token_logits))\n",
    "\n",
    "        pred = tokenizer.decode([next_token_id])\n",
    "\n",
    "        # Optional stop condition (EOS token index)\n",
    "        if pred == \"\\n\":\n",
    "            break\n",
    "        \n",
    "        # sleep(0.1)\n",
    "        input_ids.append(next_token_id)\n",
    "        print(f\"[{pred}]\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Decode IDs back to string\n",
    "    return tokenizer.decode(input_ids)\n",
    "\n",
    "# Test run\n",
    "prompt = (\n",
    "    \"BotPos=[2.23,2.25], BotRot=228, EnemyPos=[2.87,0.39], EnemyRot=87, AngleToEnemy=-29.68, AngleToEnemyScore=0.87, DistanceToEnemyScore=0.79, NearBorderArenaScore=0.42, FacingToArena=0.65. Suggested Action:\"\n",
    ")\n",
    "\n",
    "output = generate_onnx(prompt, max_new_tokens=300)\n",
    "print(f\"\\n\\nðŸ§  Output {len(output)}:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
